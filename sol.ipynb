{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "## code adopted from tf, pytorch and karpathy blog"
      ],
      "metadata": {
        "id": "yc3NMM_idT78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2FgMApgeoIw",
        "outputId": "7a55d176-1d5b-40b8-8f7f-266da482d1fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7M974RLeuAj",
        "outputId": "4a274c64-5c18-4858-8ff9-17630df48387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 400 characters in text\n",
        "print(text[:400])\n",
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')\n",
        "example_texts = ['NLPUSF', 'Assignment3']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "print(chars)\n",
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)\n",
        "ids = ids_from_chars(chars)\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "chars = chars_from_ids(ids)\n",
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVvhSfZPewpx",
        "outputId": "b8ec72ed-e9d9-4bab-9aa1-ce6e662952fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it \n",
            "65 unique characters\n",
            "<tf.RaggedTensor [[b'N', b'L', b'P', b'U', b'S', b'F'],\n",
            " [b'A', b's', b's', b'i', b'g', b'n', b'm', b'e', b'n', b't', b'3']]>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'NLPUSF', b'Assignment3'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))\n",
        "seq_length = 140\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))\n",
        "\n",
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())\n",
        "\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "split_input_target(list(\"Tensorflow\"))\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 256\n",
        "\n",
        "class NLPUSFModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OtCCq_XfRdB",
        "outputId": "f843936a-da39-40a0-f39d-d92f84a3fa80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n",
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' ' b'a' b'r' b'e' b' ' b'a' b'l' b'l' b' ' b'r' b'e' b's'\n",
            " b'o' b'l' b'v' b'e' b'd' b' ' b'r' b'a' b't' b'h' b'e' b'r' b' ' b't'\n",
            " b'o' b' ' b'd' b'i' b'e' b' ' b't' b'h' b'a' b'n' b' ' b't' b'o' b' '\n",
            " b'f'], shape=(141,), dtype=string)\n",
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to f'\n",
            "b\"amish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFir\"\n",
            "b\"st Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nS\"\n",
            "b'econd Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would'\n",
            "b' relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we'\n",
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to '\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to f'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = NLPUSFModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "M0MePKeaf-m0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGNdxQRvgExF",
        "outputId": "efe736ee-cedc-4732-a078-648a6f415764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 140, 66) # (batch_size, sequence_length, vocab_size)\n",
            "Model: \"nlpusf_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  394752    \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  16962     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 428610 (1.64 MB)\n",
            "Trainable params: 428610 (1.64 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)\n",
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K59G7B_6gOOD",
        "outputId": "596a16b7-2666-4b92-a8e8-42552100be9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b' and Derby.\\n\\nBUCKINGHAM:\\nGood time of day unto your royal grace!\\n\\nDERBY:\\nGod make your majesty joyful as you have been!\\n\\nQUEEN ELIZABETH:\\nTh'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"s$Blms&!tYx\\naOMGiXFx ?YdPBe\\np:fDjBqqd?gXt-'FG'IVlCrxZa$brURLf\\nSSsWswj.tbnWnliFWK-t-'yaVOZ,tBXwidE!ZPCJQF[UNK]YTIlSpowezjGJuE3QXQ:Ll-HN[UNK]xe3:zlOWG\"\n",
            "Prediction shape:  (64, 140, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1903095, shape=(), dtype=float32)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.043236"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "EPOCHS = 20\n",
        "# Start training your model\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gf9vdwFWgd5t",
        "outputId": "bc1b4827-37cd-4779-cbf0-22e5bec8be25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "123/123 [==============================] - 7s 20ms/step - loss: 2.8750\n",
            "Epoch 2/20\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 2.2281\n",
            "Epoch 3/20\n",
            "123/123 [==============================] - 6s 19ms/step - loss: 2.0015\n",
            "Epoch 4/20\n",
            "123/123 [==============================] - 4s 14ms/step - loss: 1.8381\n",
            "Epoch 5/20\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.7245\n",
            "Epoch 6/20\n",
            "123/123 [==============================] - 4s 14ms/step - loss: 1.6442\n",
            "Epoch 7/20\n",
            "123/123 [==============================] - 4s 18ms/step - loss: 1.5860\n",
            "Epoch 8/20\n",
            "123/123 [==============================] - 4s 14ms/step - loss: 1.5410\n",
            "Epoch 9/20\n",
            "123/123 [==============================] - 4s 15ms/step - loss: 1.5060\n",
            "Epoch 10/20\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.4769\n",
            "Epoch 11/20\n",
            "123/123 [==============================] - 4s 14ms/step - loss: 1.4536\n",
            "Epoch 12/20\n",
            "123/123 [==============================] - 6s 15ms/step - loss: 1.4330\n",
            "Epoch 13/20\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.4161\n",
            "Epoch 14/20\n",
            "123/123 [==============================] - 5s 14ms/step - loss: 1.4008\n",
            "Epoch 15/20\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.3874\n",
            "Epoch 16/20\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.3756\n",
            "Epoch 17/20\n",
            "123/123 [==============================] - 4s 13ms/step - loss: 1.3643\n",
            "Epoch 18/20\n",
            "123/123 [==============================] - 4s 15ms/step - loss: 1.3543\n",
            "Epoch 19/20\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.3456\n",
            "Epoch 20/20\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.3371\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "Ko6UtcfhgzQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "7joUigUeg4Gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['Queen:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7iz28pig44g",
        "outputId": "0b8c4086-34be-4273-ec88-eeb9a41459c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Queen:\n",
            "Take thou with our thee.\n",
            "\n",
            "LUCIO:\n",
            "To-be in my business cannot love, severe draw 'er an another beet\n",
            "fine in our country! where's thee my bootly pound\n",
            "Our still overenty. He! like a sel-welcome to before-house:\n",
            "Even therefore nog, dry if this highold,\n",
            "Some patient. Yet cannot choes my heart not what\n",
            "my life and his fool.\n",
            "\n",
            "LUCENTIO:\n",
            "Let me, here at Calive's belingly.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Harry all whom it in his shalf it fees\n",
            "This ourses as poor love.\n",
            "\n",
            "PAULINA:\n",
            "Pray'd to help you nor so unlaps!\n",
            "\n",
            "LUCENTIO:\n",
            "\n",
            "CLARENCE:\n",
            "True you to goot\n",
            "Murder was heaven, with a dount at bears;\n",
            "And to be here 'gainst my soul:\n",
            "Whate with wantance of with him with first.\n",
            "\n",
            "ANGELO:\n",
            "It is in lived grieves.\n",
            "\n",
            "PRORSTI:\n",
            "Whilst befoll to you a measure base in thyself\n",
            "Heth havour Jacks, to sleep me\n",
            "A booke shinsten, I consent him to-laugh;\n",
            "As if I am services, that late desurard;\n",
            "For may, love again.\n",
            "\n",
            "LUCIO:\n",
            "What is ever, and thy house, proUcesed upon him,\n",
            "Anthough, or thou shalt night book how as air.\n",
            "My raise mine burnt  \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.358818292617798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple beam search pseudocode, adapt this to\n",
        "function BEAM_SEARCH(RNN, start_sequence, beam_width):\n",
        "    # RNN: the recurrent neural network model for sequence generation (custom LSTM, GRU, custom Elman RNN)\n",
        "    # start_sequence: the initial part of the sequence (could be just a start symbol or set of symbols)\n",
        "    # beam_width: the number of sequences to keep at each step -- This is another hyper-parameter, play with it, as discussed in class, beam search will still provide you sub-optimal solution\n",
        "\n",
        "    Initialize an empty list `candidates` to store current sequence candidates -- One can use other datastructures, to optimize overall workeflow\n",
        "    Initialize an empty list `final_candidates` to store completed sequences\n",
        "    \n",
        "    Add start_sequence to `candidates` with its score (e.g., log likelihood)\n",
        "\n",
        "    while not all sequences in `candidates` are complete:\n",
        "        Initialize an empty list `all_expansions` for storing all possible next steps\n",
        "\n",
        "        for each sequence in `candidates`:\n",
        "            if the sequence is complete:\n",
        "                Add it to `final_candidates`\n",
        "                Continue to the next iteration\n",
        "\n",
        "            Predict the next step probabilities using RNN given the current sequence\n",
        "            Select top-k next steps (where k is the beam width) based on probabilities\n",
        "\n",
        "            for each next step in top-k:\n",
        "                Create a new sequence by appending the next step to the current sequence\n",
        "                Calculate the new sequence's score (e.g., update log likelihood)\n",
        "                Add the new sequence and its score to `all_expansions`\n",
        "\n",
        "        Sort `all_expansions` by score in descending order\n",
        "        Keep only the top `beam_width` sequences in `all_expansions`\n",
        "        Replace `candidates` with `all_expansions`\n",
        "\n",
        "    Add any remaining sequences in `candidates` to `final_candidates`\n",
        "    Sort `final_candidates` by score in descending order\n",
        "\n",
        "    return the top sequence from `final_candidates` (or top-N sequences if desired)\n",
        "\n",
        "# Usage example\n",
        "1. RNN = InitializeYourRNNModel()\n",
        "2. start_sequence = [\"<start>\"]  # Example start symbol\n",
        "3. beam_width = 5  # Example beam width\n",
        "4. best_sequence = BEAM_SEARCH(RNN, start_sequence, beam_width)\n",
        "5. print(\"Best sequence:\", best_sequence)\n",
        "Check above step on one-step this will provide you with tricks that will be useful to create beam-search"
      ],
      "metadata": {
        "id": "uAyXo9G5kZlc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Things to do\n",
        "1. Integrate custom_beamsearch with your models\n",
        "1. Optimize your hyper-parameter --> Learning rate, hidden_size, layers, optimizer, epochs, batch_size\n",
        "2. Divide dataset into train, validation, and test, once your model gets reasonable performance (lower loss), then test the story generation capability of your system\n",
        "3. Replace GRU with custom LSTM shared with you and test how it works\n",
        "4. Create custom Elman RNN (h_t = tanh(X_tW + Uh_{t-1} + b)) and compare performance across different RNNs (Custom_ElmanRNN, GRU, Custom_LSTM). Also provide loss curves for each models and saved weights.\n",
        "5. Provide statistical significance of your model\n",
        "6. Show different texts generated by your models"
      ],
      "metadata": {
        "id": "6pblhy-fhXdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def BeamSearch(RNN, start_sequence:str, beam_width:int, temperature = 1.0, gen_length=1000): #Without States\n",
        "  skip_ids = ids_from_chars(['[UNK]'])[:, None]\n",
        "  sparse_mask = tf.SparseTensor(\n",
        "      # Put a -inf at each bad index.\n",
        "      values=[-float('inf')]*len(skip_ids),\n",
        "      indices=skip_ids,\n",
        "      # Match the shape to the vocabulary\n",
        "      dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "  prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def one_step(RNN,inputs):\n",
        "    #print(\"inputs\",inputs)\n",
        "    input_chars = tf.strings.unicode_split(inputs,'UTF-8')\n",
        "    #print(\"inputs_chars\",input_chars)\n",
        "    input_ids = ids_from_chars(input_chars).to_tensor()\n",
        "    #print(\"inputs_chars\",input_ids)\n",
        "    predicted_logits = RNN(inputs=input_ids)\n",
        "    predicted_logits = predicted_logits[:,-1,:]\n",
        "    predicted_logits = predicted_logits/temperature\n",
        "    predicted_logits += prediction_mask\n",
        "    return predicted_logits #-1 is the last character in the sequence\n",
        "\n",
        "\n",
        "  next_char = tf.constant([start_sequence])\n",
        "  #print(tf.get_static_value(next_char)[0])\n",
        "  #print(len(tf.get_static_value(next_char)[0]))\n",
        "  candidates = [(0,next_char)]#,states)] # We can set our starting prob to zero because all sequences share the starting probability value\n",
        "  final_candidates = []\n",
        "  incomplete = True\n",
        "  while(incomplete):\n",
        "    all_expansions = []\n",
        "    for sequence in candidates:\n",
        "      current_seq = sequence[1]\n",
        "      if(len(tf.get_static_value(current_seq)[0])>=gen_length):\n",
        "        final_candidates.append(sequence)\n",
        "        continue\n",
        "      predicted_logits = one_step(RNN,current_seq) # Predicted Logits for the last letter in the sequence\n",
        "      softmax = tf.nn.softmax(predicted_logits,1)\n",
        "      beam_values, beam_indices = tf.nn.top_k(softmax, k=beam_width)\n",
        "      beam_ids = tf.get_static_value(beam_indices)\n",
        "      beam_scores = tf.get_static_value(beam_values)\n",
        "      for i in range(0,len(beam_ids[0])):\n",
        "        # print(beam_ids[0][i])\n",
        "        # print(chars_from_ids(beam_ids[0][i]))\n",
        "        new_sequence = tf.strings.join([current_seq]+chars_from_ids(beam_ids[0][i])) #Appends new char to sequence\n",
        "        new_score = sequence[0] + np.log(beam_scores[0][i])\n",
        "        #print(new_score,new_sequence)\n",
        "        all_expansions.append((new_score,new_sequence))#,states))\n",
        "    candidates = sorted(all_expansions, key=lambda seq: seq[0], reverse=True)[:beam_width] #Replace candidates with top N sequences where N is the beamwidth of all_expansions\n",
        "    #print(candidates[0][1])\n",
        "    incomplete = False #Check end condition\n",
        "    for seq in candidates:\n",
        "      if(len(tf.get_static_value(seq[1])[0]) < gen_length):\n",
        "        incomplete = True\n",
        "        break\n",
        "    #print(all_expansions)\n",
        "    #incomplete = False\n",
        "  final_candidates += candidates\n",
        "  return sorted(final_candidates, key=lambda seq: seq[0], reverse=True)\n"
      ],
      "metadata": {
        "id": "MVhgJNL5KEnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gru0 = NLPUSFModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=256,\n",
        "    rnn_units=256)\n",
        "\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = gru0(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "gru0.summary()\n",
        "gru0.compile(optimizer='adam', loss=loss)\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './gru0_training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "# Stops training if there is no improvement for threee consec epochs\n",
        "early_stop_callback = keras.callbacks.EarlyStopping(monitor='loss',patience=3)\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "EPOCHS = 20\n",
        "# Start training your model\n",
        "gru0_history = gru0.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback,early_stop_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJi8Zcw4MuO8",
        "outputId": "ea9c7ecd-9f6a-465b-a35c-d44138e18076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 140, 66) # (batch_size, sequence_length, vocab_size)\n",
            "Model: \"nlpusf_model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     multiple                  16896     \n",
            "                                                                 \n",
            " gru_2 (GRU)                 multiple                  394752    \n",
            "                                                                 \n",
            " dense_2 (Dense)             multiple                  16962     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 428610 (1.64 MB)\n",
            "Trainable params: 428610 (1.64 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "123/123 [==============================] - 6s 18ms/step - loss: 2.8220\n",
            "Epoch 2/20\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 2.1950\n",
            "Epoch 3/20\n",
            "123/123 [==============================] - 4s 16ms/step - loss: 1.9745\n",
            "Epoch 4/20\n",
            "123/123 [==============================] - 4s 16ms/step - loss: 1.8196\n",
            "Epoch 5/20\n",
            "123/123 [==============================] - 7s 17ms/step - loss: 1.7128\n",
            "Epoch 6/20\n",
            "123/123 [==============================] - 5s 16ms/step - loss: 1.6371\n",
            "Epoch 7/20\n",
            "123/123 [==============================] - 6s 18ms/step - loss: 1.5826\n",
            "Epoch 8/20\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.5396\n",
            "Epoch 9/20\n",
            "123/123 [==============================] - 4s 14ms/step - loss: 1.5058\n",
            "Epoch 10/20\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.4789\n",
            "Epoch 11/20\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.4565\n",
            "Epoch 12/20\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.4376\n",
            "Epoch 13/20\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.4212\n",
            "Epoch 14/20\n",
            "123/123 [==============================] - 4s 14ms/step - loss: 1.4064\n",
            "Epoch 15/20\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.3940\n",
            "Epoch 16/20\n",
            "123/123 [==============================] - 4s 15ms/step - loss: 1.3820\n",
            "Epoch 17/20\n",
            "123/123 [==============================] - 5s 15ms/step - loss: 1.3718\n",
            "Epoch 18/20\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.3621\n",
            "Epoch 19/20\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.3534\n",
            "Epoch 20/20\n",
            "123/123 [==============================] - 4s 15ms/step - loss: 1.3458\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gru1 = NLPUSFModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=256,\n",
        "    rnn_units=256)\n",
        "\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = gru1(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "gru1.summary()\n",
        "gru1.compile(optimizer='adam', loss=loss)\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './gru1_training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "# Stops training if there is no improvement for threee consec epochs\n",
        "early_stop_callback = keras.callbacks.EarlyStopping(monitor='loss',patience=3)\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "EPOCHS = 100\n",
        "# Start training your model\n",
        "gru1_history = gru1.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback,early_stop_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6J2EfG1M4jh",
        "outputId": "b5cebfc7-86c0-4833-d284-495785d3d829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 140, 66) # (batch_size, sequence_length, vocab_size)\n",
            "Model: \"nlpusf_model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     multiple                  16896     \n",
            "                                                                 \n",
            " gru_3 (GRU)                 multiple                  394752    \n",
            "                                                                 \n",
            " dense_3 (Dense)             multiple                  16962     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 428610 (1.64 MB)\n",
            "Trainable params: 428610 (1.64 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "123/123 [==============================] - 6s 16ms/step - loss: 2.9028\n",
            "Epoch 2/100\n",
            "123/123 [==============================] - 4s 15ms/step - loss: 2.2450\n",
            "Epoch 3/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 2.0248\n",
            "Epoch 4/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.8610\n",
            "Epoch 5/100\n",
            "123/123 [==============================] - 5s 16ms/step - loss: 1.7453\n",
            "Epoch 6/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.6631\n",
            "Epoch 7/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.6028\n",
            "Epoch 8/100\n",
            "123/123 [==============================] - 4s 15ms/step - loss: 1.5559\n",
            "Epoch 9/100\n",
            "123/123 [==============================] - 4s 14ms/step - loss: 1.5194\n",
            "Epoch 10/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.4897\n",
            "Epoch 11/100\n",
            "123/123 [==============================] - 4s 14ms/step - loss: 1.4646\n",
            "Epoch 12/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.4432\n",
            "Epoch 13/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.4246\n",
            "Epoch 14/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.4087\n",
            "Epoch 15/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.3940\n",
            "Epoch 16/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.3811\n",
            "Epoch 17/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.3696\n",
            "Epoch 18/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.3581\n",
            "Epoch 19/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.3490\n",
            "Epoch 20/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.3398\n",
            "Epoch 21/100\n",
            "123/123 [==============================] - 3s 15ms/step - loss: 1.3320\n",
            "Epoch 22/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.3239\n",
            "Epoch 23/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.3163\n",
            "Epoch 24/100\n",
            "123/123 [==============================] - 4s 14ms/step - loss: 1.3092\n",
            "Epoch 25/100\n",
            "123/123 [==============================] - 4s 14ms/step - loss: 1.3032\n",
            "Epoch 26/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.2971\n",
            "Epoch 27/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.2907\n",
            "Epoch 28/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.2857\n",
            "Epoch 29/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.2797\n",
            "Epoch 30/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.2752\n",
            "Epoch 31/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.2703\n",
            "Epoch 32/100\n",
            "123/123 [==============================] - 4s 14ms/step - loss: 1.2654\n",
            "Epoch 33/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.2609\n",
            "Epoch 34/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.2560\n",
            "Epoch 35/100\n",
            "123/123 [==============================] - 4s 14ms/step - loss: 1.2523\n",
            "Epoch 36/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.2477\n",
            "Epoch 37/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.2440\n",
            "Epoch 38/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.2401\n",
            "Epoch 39/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.2362\n",
            "Epoch 40/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.2322\n",
            "Epoch 41/100\n",
            "123/123 [==============================] - 4s 14ms/step - loss: 1.2294\n",
            "Epoch 42/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.2253\n",
            "Epoch 43/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.2218\n",
            "Epoch 44/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.2187\n",
            "Epoch 45/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.2160\n",
            "Epoch 46/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.2124\n",
            "Epoch 47/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.2094\n",
            "Epoch 48/100\n",
            "123/123 [==============================] - 4s 14ms/step - loss: 1.2061\n",
            "Epoch 49/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.2035\n",
            "Epoch 50/100\n",
            "123/123 [==============================] - 4s 14ms/step - loss: 1.2001\n",
            "Epoch 51/100\n",
            "123/123 [==============================] - 4s 14ms/step - loss: 1.1970\n",
            "Epoch 52/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.1944\n",
            "Epoch 53/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.1915\n",
            "Epoch 54/100\n",
            "123/123 [==============================] - 4s 14ms/step - loss: 1.1890\n",
            "Epoch 55/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.1858\n",
            "Epoch 56/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.1838\n",
            "Epoch 57/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.1811\n",
            "Epoch 58/100\n",
            "123/123 [==============================] - 4s 14ms/step - loss: 1.1784\n",
            "Epoch 59/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.1759\n",
            "Epoch 60/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.1736\n",
            "Epoch 61/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.1712\n",
            "Epoch 62/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.1687\n",
            "Epoch 63/100\n",
            "123/123 [==============================] - 4s 14ms/step - loss: 1.1660\n",
            "Epoch 64/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.1643\n",
            "Epoch 65/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.1614\n",
            "Epoch 66/100\n",
            "123/123 [==============================] - 4s 14ms/step - loss: 1.1595\n",
            "Epoch 67/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.1572\n",
            "Epoch 68/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.1549\n",
            "Epoch 69/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.1535\n",
            "Epoch 70/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.1516\n",
            "Epoch 71/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.1492\n",
            "Epoch 72/100\n",
            "123/123 [==============================] - 4s 15ms/step - loss: 1.1461\n",
            "Epoch 73/100\n",
            "123/123 [==============================] - 4s 14ms/step - loss: 1.1447\n",
            "Epoch 74/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.1425\n",
            "Epoch 75/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.1416\n",
            "Epoch 76/100\n",
            "123/123 [==============================] - 4s 14ms/step - loss: 1.1392\n",
            "Epoch 77/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.1379\n",
            "Epoch 78/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.1357\n",
            "Epoch 79/100\n",
            "123/123 [==============================] - 4s 14ms/step - loss: 1.1348\n",
            "Epoch 80/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.1327\n",
            "Epoch 81/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.1312\n",
            "Epoch 82/100\n",
            "123/123 [==============================] - 4s 14ms/step - loss: 1.1293\n",
            "Epoch 83/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.1274\n",
            "Epoch 84/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.1264\n",
            "Epoch 85/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.1241\n",
            "Epoch 86/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.1235\n",
            "Epoch 87/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.1214\n",
            "Epoch 88/100\n",
            "123/123 [==============================] - 4s 14ms/step - loss: 1.1201\n",
            "Epoch 89/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.1191\n",
            "Epoch 90/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.1176\n",
            "Epoch 91/100\n",
            "123/123 [==============================] - 4s 14ms/step - loss: 1.1164\n",
            "Epoch 92/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.1151\n",
            "Epoch 93/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.1145\n",
            "Epoch 94/100\n",
            "123/123 [==============================] - 4s 14ms/step - loss: 1.1124\n",
            "Epoch 95/100\n",
            "123/123 [==============================] - 3s 13ms/step - loss: 1.1121\n",
            "Epoch 96/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.1107\n",
            "Epoch 97/100\n",
            "123/123 [==============================] - 4s 14ms/step - loss: 1.1093\n",
            "Epoch 98/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.1077\n",
            "Epoch 99/100\n",
            "123/123 [==============================] - 3s 14ms/step - loss: 1.1077\n",
            "Epoch 100/100\n",
            "123/123 [==============================] - 4s 14ms/step - loss: 1.1064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gru = NLPUSFModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=512,\n",
        "    rnn_units=512)\n",
        "\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = gru(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "gru.summary()\n",
        "gru.compile(optimizer='adam', loss=loss)\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './gru_training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "# Stops training if there is no improvement for threee consec epochs\n",
        "early_stop_callback = keras.callbacks.EarlyStopping(monitor='loss',patience=3)\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "EPOCHS = 100\n",
        "# Start training your model\n",
        "gru_history = gru.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback,early_stop_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bm5YffcbKI0l",
        "outputId": "e87d10b1-b458-4971-c41d-b06f9d140d30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 140, 66) # (batch_size, sequence_length, vocab_size)\n",
            "Model: \"nlpusf_model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     multiple                  33792     \n",
            "                                                                 \n",
            " gru_1 (GRU)                 multiple                  1575936   \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  33858     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1643586 (6.27 MB)\n",
            "Trainable params: 1643586 (6.27 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "123/123 [==============================] - 9s 39ms/step - loss: 2.7517\n",
            "Epoch 2/100\n",
            "123/123 [==============================] - 6s 35ms/step - loss: 2.0866\n",
            "Epoch 3/100\n",
            "123/123 [==============================] - 6s 35ms/step - loss: 1.8266\n",
            "Epoch 4/100\n",
            "123/123 [==============================] - 6s 36ms/step - loss: 1.6598\n",
            "Epoch 5/100\n",
            "123/123 [==============================] - 6s 36ms/step - loss: 1.5558\n",
            "Epoch 6/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 1.4864\n",
            "Epoch 7/100\n",
            "123/123 [==============================] - 6s 38ms/step - loss: 1.4359\n",
            "Epoch 8/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 1.3979\n",
            "Epoch 9/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 1.3672\n",
            "Epoch 10/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 1.3416\n",
            "Epoch 11/100\n",
            "123/123 [==============================] - 7s 43ms/step - loss: 1.3191\n",
            "Epoch 12/100\n",
            "123/123 [==============================] - 6s 38ms/step - loss: 1.2989\n",
            "Epoch 13/100\n",
            "123/123 [==============================] - 6s 38ms/step - loss: 1.2816\n",
            "Epoch 14/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 1.2637\n",
            "Epoch 15/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 1.2491\n",
            "Epoch 16/100\n",
            "123/123 [==============================] - 7s 37ms/step - loss: 1.2334\n",
            "Epoch 17/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 1.2201\n",
            "Epoch 18/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 1.2054\n",
            "Epoch 19/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 1.1909\n",
            "Epoch 20/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 1.1776\n",
            "Epoch 21/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 1.1642\n",
            "Epoch 22/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 1.1506\n",
            "Epoch 23/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 1.1376\n",
            "Epoch 24/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 1.1230\n",
            "Epoch 25/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 1.1101\n",
            "Epoch 26/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 1.0961\n",
            "Epoch 27/100\n",
            "123/123 [==============================] - 7s 41ms/step - loss: 1.0828\n",
            "Epoch 28/100\n",
            "123/123 [==============================] - 11s 40ms/step - loss: 1.0694\n",
            "Epoch 29/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 1.0557\n",
            "Epoch 30/100\n",
            "123/123 [==============================] - 7s 37ms/step - loss: 1.0412\n",
            "Epoch 31/100\n",
            "123/123 [==============================] - 7s 38ms/step - loss: 1.0283\n",
            "Epoch 32/100\n",
            "123/123 [==============================] - 7s 38ms/step - loss: 1.0136\n",
            "Epoch 33/100\n",
            "123/123 [==============================] - 6s 38ms/step - loss: 1.0016\n",
            "Epoch 34/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.9872\n",
            "Epoch 35/100\n",
            "123/123 [==============================] - 7s 37ms/step - loss: 0.9735\n",
            "Epoch 36/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.9600\n",
            "Epoch 37/100\n",
            "123/123 [==============================] - 7s 37ms/step - loss: 0.9475\n",
            "Epoch 38/100\n",
            "123/123 [==============================] - 7s 37ms/step - loss: 0.9353\n",
            "Epoch 39/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.9217\n",
            "Epoch 40/100\n",
            "123/123 [==============================] - 7s 37ms/step - loss: 0.9101\n",
            "Epoch 41/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.9002\n",
            "Epoch 42/100\n",
            "123/123 [==============================] - 6s 38ms/step - loss: 0.8881\n",
            "Epoch 43/100\n",
            "123/123 [==============================] - 7s 37ms/step - loss: 0.8782\n",
            "Epoch 44/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.8692\n",
            "Epoch 45/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.8595\n",
            "Epoch 46/100\n",
            "123/123 [==============================] - 7s 37ms/step - loss: 0.8516\n",
            "Epoch 47/100\n",
            "123/123 [==============================] - 6s 38ms/step - loss: 0.8432\n",
            "Epoch 48/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.8355\n",
            "Epoch 49/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.8278\n",
            "Epoch 50/100\n",
            "123/123 [==============================] - 7s 38ms/step - loss: 0.8210\n",
            "Epoch 51/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.8146\n",
            "Epoch 52/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.8091\n",
            "Epoch 53/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.8035\n",
            "Epoch 54/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.7975\n",
            "Epoch 55/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.7922\n",
            "Epoch 56/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.7866\n",
            "Epoch 57/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.7822\n",
            "Epoch 58/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.7783\n",
            "Epoch 59/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.7728\n",
            "Epoch 60/100\n",
            "123/123 [==============================] - 7s 37ms/step - loss: 0.7684\n",
            "Epoch 61/100\n",
            "123/123 [==============================] - 6s 38ms/step - loss: 0.7643\n",
            "Epoch 62/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.7613\n",
            "Epoch 63/100\n",
            "123/123 [==============================] - 6s 38ms/step - loss: 0.7588\n",
            "Epoch 64/100\n",
            "123/123 [==============================] - 6s 38ms/step - loss: 0.7563\n",
            "Epoch 65/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.7530\n",
            "Epoch 66/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.7500\n",
            "Epoch 67/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.7447\n",
            "Epoch 68/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.7428\n",
            "Epoch 69/100\n",
            "123/123 [==============================] - 6s 38ms/step - loss: 0.7393\n",
            "Epoch 70/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.7412\n",
            "Epoch 71/100\n",
            "123/123 [==============================] - 6s 38ms/step - loss: 0.7346\n",
            "Epoch 72/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.7341\n",
            "Epoch 73/100\n",
            "123/123 [==============================] - 6s 38ms/step - loss: 0.7307\n",
            "Epoch 74/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.7283\n",
            "Epoch 75/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.7256\n",
            "Epoch 76/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.7233\n",
            "Epoch 77/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.7193\n",
            "Epoch 78/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.7209\n",
            "Epoch 79/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.7168\n",
            "Epoch 80/100\n",
            "123/123 [==============================] - 6s 38ms/step - loss: 0.7142\n",
            "Epoch 81/100\n",
            "123/123 [==============================] - 6s 38ms/step - loss: 0.7145\n",
            "Epoch 82/100\n",
            "123/123 [==============================] - 6s 38ms/step - loss: 0.7112\n",
            "Epoch 83/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.7099\n",
            "Epoch 84/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.7105\n",
            "Epoch 85/100\n",
            "123/123 [==============================] - 7s 37ms/step - loss: 0.7080\n",
            "Epoch 86/100\n",
            "123/123 [==============================] - 7s 37ms/step - loss: 0.7059\n",
            "Epoch 87/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.7020\n",
            "Epoch 88/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.7017\n",
            "Epoch 89/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.6996\n",
            "Epoch 90/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.6975\n",
            "Epoch 91/100\n",
            "123/123 [==============================] - 6s 38ms/step - loss: 0.6988\n",
            "Epoch 92/100\n",
            "123/123 [==============================] - 7s 38ms/step - loss: 0.6967\n",
            "Epoch 93/100\n",
            "123/123 [==============================] - 6s 38ms/step - loss: 0.6969\n",
            "Epoch 94/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.6931\n",
            "Epoch 95/100\n",
            "123/123 [==============================] - 7s 37ms/step - loss: 0.6939\n",
            "Epoch 96/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.6900\n",
            "Epoch 97/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.6884\n",
            "Epoch 98/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.6858\n",
            "Epoch 99/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.6892\n",
            "Epoch 100/100\n",
            "123/123 [==============================] - 6s 37ms/step - loss: 0.6879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final = BeamSearch(gru,\"Queen: \",5,gen_length=500)"
      ],
      "metadata": {
        "id": "-isqp_lpKQ8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final0 = BeamSearch(gru0,\"Queen: \",5,gen_length=500)\n",
        "final1 = BeamSearch(gru1,\"Queen: \",5,gen_length=500)\n"
      ],
      "metadata": {
        "id": "3tFzwj85Ox5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(final[0])\n",
        "print(final0[0][1][0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print(final1[1][1][0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print(final[2][1][0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNrUVGMHKU01",
        "outputId": "49d6c852-1a52-4726-d8bb-da790fcbd3ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Queen: therefore he shall have thee to the prince,\n",
            "And therefore he shall have thee to the prince,\n",
            "And therefore he shall have thee to the prince,\n",
            "And therefore he shall have thee to the prince,\n",
            "And therefore he shall have thee to the prince,\n",
            "And therefore he shall have thee to the prince,\n",
            "And therefore he shall have thee to the prince,\n",
            "And therefore he shall have thee to the prince,\n",
            "And therefore he shall have thee to the prince,\n",
            "And therefore he shall have thee to the prince,\n",
            "And there is the  \n",
            "\n",
            "________________________________________________________________________________\n",
            "Queen: 'tis no more down.\n",
            "\n",
            "LEONTES:\n",
            "What is the news?\n",
            "\n",
            "BUCKINGHAM:\n",
            "My lord, I have no more remedy.\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "My lord, this is the prince, which shall be thy conscience.\n",
            "\n",
            "KING RICHARD III:\n",
            "Why, thou dost thou dead, and thou shalt not never see\n",
            "The present death and honour of the death.\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "My lord, this is nothing; and therefore comes all.\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "My lord, this is the prince, which should call thee speak.\n",
            "\n",
            "KING RICHARD III:\n",
            "Why, thou hast no more of yours.\n",
            "\n",
            "LADY AN \n",
            "\n",
            "________________________________________________________________________________\n",
            "Queen: dost thou upon thy face,\n",
            "Than thou wert born to see the field.\n",
            "\n",
            "LEONTES:\n",
            "I thank your grace.\n",
            "\n",
            "GLOUCESTER:\n",
            "\n",
            "CLARENCE:\n",
            "\n",
            "KING EDWARD IV:\n",
            "There's my father York and Salisbury,\n",
            "So soon as you are, my lord,--\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "O, thou art deceived in the Tower.\n",
            "\n",
            "KING EDWARD IV:\n",
            "But stay, good Grumio, my lord.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "There is no less.\n",
            "\n",
            "KING RICHARD III:\n",
            "The advancement of your conscience, and me answer.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "O heavens! O heavens!\n",
            "What then?\n",
            "\n",
            "First Servingman:\n",
            "A strange law \n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLSTMCell(keras.layers.Layer):\n",
        "  def __init__(self, units, **kwargs):\n",
        "    super(CustomLSTMCell, self).__init__(**kwargs)\n",
        "    self.units = units\n",
        "    self.state_size = [units, units]  # Hidden state size and cell state size\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    input_dim = input_shape[-1]\n",
        "    # One can play with init to stabalize learning, remember what we discussed for MLP\n",
        "    # As described in class LSTM is simply 4 different RNNs (h_t = sigma(Wx_t + Uh_{t-1} + b)) working in parallel, but connected jointly.\n",
        "    # Weights for the input gate\n",
        "    self.W_i = self.add_weight(shape=(input_dim, self.units), initializer='random_normal', name='W_i')\n",
        "    self.U_i = self.add_weight(shape=(self.units, self.units), initializer='random_normal', name='U_i')\n",
        "    self.b_i = self.add_weight(shape=(self.units,), initializer='zeros', name='b_i')\n",
        "\n",
        "    # Weights for the forget gate\n",
        "    self.W_f = self.add_weight(shape=(input_dim, self.units), initializer='random_normal', name='W_f')\n",
        "    self.U_f = self.add_weight(shape=(self.units, self.units), initializer='random_normal', name='U_f')\n",
        "    self.b_f = self.add_weight(shape=(self.units,), initializer='zeros', name='b_f')\n",
        "\n",
        "    # Weights for the cell state\n",
        "    self.W_c = self.add_weight(shape=(input_dim, self.units), initializer='random_normal', name='W_c')\n",
        "    self.U_c = self.add_weight(shape=(self.units, self.units), initializer='random_normal', name='U_c')\n",
        "    self.b_c = self.add_weight(shape=(self.units,), initializer='zeros', name='b_c')\n",
        "\n",
        "    # Weights for the output gate\n",
        "    self.W_o = self.add_weight(shape=(input_dim, self.units), initializer='random_normal', name='W_o')\n",
        "    self.U_o = self.add_weight(shape=(self.units, self.units), initializer='random_normal', name='U_o')\n",
        "    self.b_o = self.add_weight(shape=(self.units,), initializer='zeros', name='b_o')\n",
        "\n",
        "    super(CustomLSTMCell, self).build(input_shape)\n",
        "\n",
        "  def call(self, inputs, states, return_state=None,training=None):\n",
        "    #print(\"called\")\n",
        "    h_tm1, c_tm1 = states  # Previous state\n",
        "    # Input gate\n",
        "    i = tf.sigmoid(tf.matmul(inputs, self.W_i) + tf.matmul(h_tm1, self.U_i) + self.b_i)\n",
        "    # Forget gate\n",
        "    f = tf.sigmoid(tf.matmul(inputs, self.W_f) + tf.matmul(h_tm1, self.U_f) + self.b_f)\n",
        "\n",
        "    # Cell state\n",
        "    c_ = tf.tanh(tf.matmul(inputs, self.W_c) + tf.matmul(h_tm1, self.U_c) + self.b_c)\n",
        "    c = f * c_tm1 + i * c_\n",
        "\n",
        "    # Output gate\n",
        "    o = tf.sigmoid(tf.matmul(inputs, self.W_o) + tf.matmul(h_tm1, self.U_o) + self.b_o)\n",
        "    # New hidden state\n",
        "    h = o * tf.tanh(c)\n",
        "    return h, [h, c]"
      ],
      "metadata": {
        "id": "SxA5AEuIKXWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 66\n",
        "embedding_dim = 512\n",
        "rnn_units = 512 # Number of LSTM units\n",
        "input_shape = (None, embedding_dim)  # Example input shape (timesteps, features)\n",
        "# Create the LSTM layer using the custom cell\n",
        "lstm_layer = keras.layers.RNN(CustomLSTMCell(rnn_units), input_shape=input_shape,return_sequences=True)\n",
        "lstm = keras.Sequential([\n",
        "  keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "  lstm_layer,\n",
        "  keras.layers.Dense(vocab_size)  # Example output layer\n",
        "])\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "lstm.compile(optimizer='adam', loss=loss)\n",
        "lstm.summary()\n"
      ],
      "metadata": {
        "id": "Yr6hQ7TTKu6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './lstm_training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "# Stops training if there is no improvement for threee consec epochs\n",
        "early_stop_callback = keras.callbacks.EarlyStopping(monitor='loss',patience=3)\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "EPOCHS = 100\n",
        "# Start training your model\n",
        "lstm_history = lstm.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback,early_stop_callback])"
      ],
      "metadata": {
        "id": "km_sTa6gKxGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "858aa7ad-996a-4dbd-e7f2-bed52f1d7f28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'lstm' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-8ede43b9d08f>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Start training your model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mlstm_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearly_stop_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'lstm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 66\n",
        "embedding_dim = 256\n",
        "rnn_units = 512 # Number of units\n",
        "input_shape = (None, embedding_dim)  # Example input shape (timesteps, features)\n",
        "# Create the Elman layer using SimpleRNN\n",
        "elman_layerR512 = keras.layers.SimpleRNN(rnn_units,input_shape=input_shape,return_sequences=True)\n",
        "elmanR512 = keras.Sequential([\n",
        "  keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "  elman_layerR512,\n",
        "  keras.layers.Dense(vocab_size)  # Example output layer\n",
        "])\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "elmanR512.compile(optimizer='adam', loss=loss)\n",
        "elmanR512.summary()\n",
        "\n",
        "checkpoint_dir = './elmanR512_training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "# Stops training if there is no improvement for threee consec epochs\n",
        "early_stop_callback = keras.callbacks.EarlyStopping(monitor='loss',patience=3)\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "EPOCHS = 100\n",
        "# Start training your model\n",
        "elmanR512_history = elmanR512.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback,early_stop_callback])"
      ],
      "metadata": {
        "id": "K3yfnlFGK1C5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba6c537d-9cfd-4a79-ff4d-2370987246a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, None, 256)         16896     \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, None, 512)         393728    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, None, 66)          33858     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 444482 (1.70 MB)\n",
            "Trainable params: 444482 (1.70 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "123/123 [==============================] - 18s 100ms/step - loss: 2.7443\n",
            "Epoch 2/100\n",
            "123/123 [==============================] - 14s 98ms/step - loss: 2.1452\n",
            "Epoch 3/100\n",
            "123/123 [==============================] - 14s 97ms/step - loss: 1.9630\n",
            "Epoch 4/100\n",
            "123/123 [==============================] - 14s 98ms/step - loss: 1.8328\n",
            "Epoch 5/100\n",
            "123/123 [==============================] - 14s 97ms/step - loss: 1.7376\n",
            "Epoch 6/100\n",
            "123/123 [==============================] - 14s 98ms/step - loss: 1.6639\n",
            "Epoch 7/100\n",
            "123/123 [==============================] - 15s 106ms/step - loss: 1.6094\n",
            "Epoch 8/100\n",
            "123/123 [==============================] - 14s 99ms/step - loss: 1.5658\n",
            "Epoch 9/100\n",
            "123/123 [==============================] - 14s 98ms/step - loss: 1.5311\n",
            "Epoch 10/100\n",
            "123/123 [==============================] - 14s 99ms/step - loss: 1.5010\n",
            "Epoch 11/100\n",
            "123/123 [==============================] - 14s 98ms/step - loss: 1.4777\n",
            "Epoch 12/100\n",
            "123/123 [==============================] - 14s 97ms/step - loss: 1.4572\n",
            "Epoch 13/100\n",
            "123/123 [==============================] - 14s 97ms/step - loss: 1.4371\n",
            "Epoch 14/100\n",
            "123/123 [==============================] - 14s 98ms/step - loss: 1.4227\n",
            "Epoch 15/100\n",
            "123/123 [==============================] - 14s 97ms/step - loss: 1.4076\n",
            "Epoch 16/100\n",
            "123/123 [==============================] - 14s 96ms/step - loss: 1.3936\n",
            "Epoch 17/100\n",
            "123/123 [==============================] - 13s 96ms/step - loss: 1.3827\n",
            "Epoch 18/100\n",
            "123/123 [==============================] - 14s 96ms/step - loss: 1.3709\n",
            "Epoch 19/100\n",
            "123/123 [==============================] - 14s 98ms/step - loss: 1.3615\n",
            "Epoch 20/100\n",
            "123/123 [==============================] - 14s 97ms/step - loss: 1.3534\n",
            "Epoch 21/100\n",
            "123/123 [==============================] - 13s 96ms/step - loss: 1.3421\n",
            "Epoch 22/100\n",
            "123/123 [==============================] - 14s 97ms/step - loss: 1.3341\n",
            "Epoch 23/100\n",
            "123/123 [==============================] - 14s 99ms/step - loss: 1.3263\n",
            "Epoch 24/100\n",
            "123/123 [==============================] - 14s 96ms/step - loss: 1.3188\n",
            "Epoch 25/100\n",
            "123/123 [==============================] - 14s 97ms/step - loss: 1.3122\n",
            "Epoch 26/100\n",
            "123/123 [==============================] - 13s 96ms/step - loss: 1.3036\n",
            "Epoch 27/100\n",
            "123/123 [==============================] - 14s 95ms/step - loss: 1.2973\n",
            "Epoch 28/100\n",
            "123/123 [==============================] - 14s 97ms/step - loss: 1.2929\n",
            "Epoch 29/100\n",
            "123/123 [==============================] - 14s 98ms/step - loss: 1.2856\n",
            "Epoch 30/100\n",
            "123/123 [==============================] - 14s 97ms/step - loss: 1.2810\n",
            "Epoch 31/100\n",
            "123/123 [==============================] - 14s 98ms/step - loss: 1.2745\n",
            "Epoch 32/100\n",
            "123/123 [==============================] - 14s 98ms/step - loss: 1.2679\n",
            "Epoch 33/100\n",
            "123/123 [==============================] - 14s 98ms/step - loss: 1.2622\n",
            "Epoch 34/100\n",
            "123/123 [==============================] - 14s 95ms/step - loss: 1.2589\n",
            "Epoch 35/100\n",
            "123/123 [==============================] - 14s 97ms/step - loss: 1.2515\n",
            "Epoch 36/100\n",
            "123/123 [==============================] - 14s 98ms/step - loss: 1.2476\n",
            "Epoch 37/100\n",
            "123/123 [==============================] - 13s 93ms/step - loss: 1.2422\n",
            "Epoch 38/100\n",
            "123/123 [==============================] - 14s 98ms/step - loss: 1.2383\n",
            "Epoch 39/100\n",
            "123/123 [==============================] - 14s 99ms/step - loss: 1.2336\n",
            "Epoch 40/100\n",
            "123/123 [==============================] - 14s 99ms/step - loss: 1.2297\n",
            "Epoch 41/100\n",
            "123/123 [==============================] - 16s 109ms/step - loss: 1.2242\n",
            "Epoch 42/100\n",
            "123/123 [==============================] - 14s 100ms/step - loss: 1.2212\n",
            "Epoch 43/100\n",
            "123/123 [==============================] - 14s 95ms/step - loss: 1.2158\n",
            "Epoch 44/100\n",
            "123/123 [==============================] - 14s 98ms/step - loss: 1.2119\n",
            "Epoch 45/100\n",
            "123/123 [==============================] - 14s 97ms/step - loss: 1.2060\n",
            "Epoch 46/100\n",
            "123/123 [==============================] - 14s 98ms/step - loss: 1.2042\n",
            "Epoch 47/100\n",
            "123/123 [==============================] - 14s 96ms/step - loss: 1.2018\n",
            "Epoch 48/100\n",
            "123/123 [==============================] - 14s 100ms/step - loss: 1.1978\n",
            "Epoch 49/100\n",
            "123/123 [==============================] - 14s 98ms/step - loss: 1.1911\n",
            "Epoch 50/100\n",
            "123/123 [==============================] - 14s 98ms/step - loss: 1.1875\n",
            "Epoch 51/100\n",
            "123/123 [==============================] - 14s 99ms/step - loss: 1.1826\n",
            "Epoch 52/100\n",
            "123/123 [==============================] - 14s 101ms/step - loss: 1.1822\n",
            "Epoch 53/100\n",
            "123/123 [==============================] - 14s 99ms/step - loss: 1.1788\n",
            "Epoch 54/100\n",
            "123/123 [==============================] - 14s 98ms/step - loss: 1.1735\n",
            "Epoch 55/100\n",
            "123/123 [==============================] - 14s 100ms/step - loss: 1.1694\n",
            "Epoch 56/100\n",
            "123/123 [==============================] - 14s 101ms/step - loss: 1.1693\n",
            "Epoch 57/100\n",
            "123/123 [==============================] - 15s 100ms/step - loss: 1.1634\n",
            "Epoch 58/100\n",
            "123/123 [==============================] - 14s 97ms/step - loss: 1.1625\n",
            "Epoch 59/100\n",
            "123/123 [==============================] - 14s 99ms/step - loss: 1.1610\n",
            "Epoch 60/100\n",
            "123/123 [==============================] - 14s 98ms/step - loss: 1.1552\n",
            "Epoch 61/100\n",
            "123/123 [==============================] - 14s 98ms/step - loss: 1.1512\n",
            "Epoch 62/100\n",
            "123/123 [==============================] - 14s 97ms/step - loss: 1.1498\n",
            "Epoch 63/100\n",
            "123/123 [==============================] - 14s 96ms/step - loss: 1.1450\n",
            "Epoch 64/100\n",
            "123/123 [==============================] - 14s 98ms/step - loss: 1.1442\n",
            "Epoch 65/100\n",
            "123/123 [==============================] - 14s 99ms/step - loss: 1.1438\n",
            "Epoch 66/100\n",
            "123/123 [==============================] - 14s 100ms/step - loss: 1.1387\n",
            "Epoch 67/100\n",
            "123/123 [==============================] - 14s 99ms/step - loss: 1.1359\n",
            "Epoch 68/100\n",
            "123/123 [==============================] - 14s 98ms/step - loss: 1.1337\n",
            "Epoch 69/100\n",
            "123/123 [==============================] - 14s 97ms/step - loss: 1.1325\n",
            "Epoch 70/100\n",
            "123/123 [==============================] - 14s 99ms/step - loss: 1.1304\n",
            "Epoch 71/100\n",
            "123/123 [==============================] - 14s 99ms/step - loss: 1.1265\n",
            "Epoch 72/100\n",
            "123/123 [==============================] - 14s 97ms/step - loss: 1.1247\n",
            "Epoch 73/100\n",
            "123/123 [==============================] - 14s 97ms/step - loss: 1.1240\n",
            "Epoch 74/100\n",
            "123/123 [==============================] - 14s 97ms/step - loss: 1.1219\n",
            "Epoch 75/100\n",
            "123/123 [==============================] - 15s 109ms/step - loss: 1.1182\n",
            "Epoch 76/100\n",
            "123/123 [==============================] - 14s 100ms/step - loss: 1.1165\n",
            "Epoch 77/100\n",
            "123/123 [==============================] - 14s 100ms/step - loss: 1.1161\n",
            "Epoch 78/100\n",
            "123/123 [==============================] - 14s 98ms/step - loss: 1.1137\n",
            "Epoch 79/100\n",
            "123/123 [==============================] - 14s 99ms/step - loss: 1.1138\n",
            "Epoch 80/100\n",
            "123/123 [==============================] - 14s 98ms/step - loss: 1.1147\n",
            "Epoch 81/100\n",
            "123/123 [==============================] - 14s 98ms/step - loss: 1.1106\n",
            "Epoch 82/100\n",
            "123/123 [==============================] - 14s 93ms/step - loss: 1.1056\n",
            "Epoch 83/100\n",
            "123/123 [==============================] - 14s 99ms/step - loss: 1.1069\n",
            "Epoch 84/100\n",
            "123/123 [==============================] - 14s 97ms/step - loss: 1.1058\n",
            "Epoch 85/100\n",
            "123/123 [==============================] - 14s 98ms/step - loss: 1.1029\n",
            "Epoch 86/100\n",
            "123/123 [==============================] - 14s 97ms/step - loss: 1.1022\n",
            "Epoch 87/100\n",
            "123/123 [==============================] - 14s 99ms/step - loss: 1.1016\n",
            "Epoch 88/100\n",
            "123/123 [==============================] - 14s 97ms/step - loss: 1.0988\n",
            "Epoch 89/100\n",
            "123/123 [==============================] - 15s 110ms/step - loss: 1.1000\n",
            "Epoch 90/100\n",
            "123/123 [==============================] - 24s 171ms/step - loss: 1.0965\n",
            "Epoch 91/100\n",
            "123/123 [==============================] - 16s 115ms/step - loss: 1.0975\n",
            "Epoch 92/100\n",
            "123/123 [==============================] - 19s 141ms/step - loss: 1.0954\n",
            "Epoch 93/100\n",
            "123/123 [==============================] - 14s 99ms/step - loss: 1.0948\n",
            "Epoch 94/100\n",
            "123/123 [==============================] - 14s 97ms/step - loss: 1.0921\n",
            "Epoch 95/100\n",
            "123/123 [==============================] - 14s 98ms/step - loss: 1.0929\n",
            "Epoch 96/100\n",
            "123/123 [==============================] - 15s 105ms/step - loss: 1.0917\n",
            "Epoch 97/100\n",
            "123/123 [==============================] - 14s 99ms/step - loss: 1.0908\n",
            "Epoch 98/100\n",
            "123/123 [==============================] - 15s 104ms/step - loss: 1.0921\n",
            "Epoch 99/100\n",
            "123/123 [==============================] - 15s 102ms/step - loss: 1.0900\n",
            "Epoch 100/100\n",
            "123/123 [==============================] - 16s 110ms/step - loss: 1.0880\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "elman_final = BeamSearch(elmanR512,\"Queen: \",5,gen_length=500)"
      ],
      "metadata": {
        "id": "hf4ZTFHXYja2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(elman_final[0][1][0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXbI11t8YrEt",
        "outputId": "4e96ce28-609f-4492-ac2d-66eca0497d62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Queen: they are not your father's death,\n",
            "And that the queen is out of thine own life,\n",
            "And that the queen is out of thine own life,\n",
            "And that the queen is out of thine own life,\n",
            "And that the queen is out of thine own life,\n",
            "And that the queen is out of thine own life,\n",
            "And that the queen is out of thine own life,\n",
            "And that the queen is out of thine own life,\n",
            "And that the queen is out of thine own life,\n",
            "And that the queen is out of thine own life,\n",
            "And that the queen is out of thine own life,\n",
            "And that  \n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 66\n",
        "embedding_dim = 256\n",
        "rnn_units = 128 # Number of units\n",
        "input_shape = (None, embedding_dim)  # Example input shape (timesteps, features)\n",
        "# Create the Elman layer using SimpleRNN\n",
        "elman_layerR128 = keras.layers.SimpleRNN(rnn_units,input_shape=input_shape,return_sequences=True)\n",
        "elmanR128 = keras.Sequential([\n",
        "  keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "  elman_layerR128,\n",
        "  keras.layers.Dense(vocab_size)  # Example output layer\n",
        "])\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "elmanR128.compile(optimizer='adam', loss=loss)\n",
        "elmanR128.summary()\n",
        "\n",
        "checkpoint_dir = './elmanR128_training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "# Stops training if there is no improvement for threee consec epochs\n",
        "early_stop_callback = keras.callbacks.EarlyStopping(monitor='loss',patience=3)\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "EPOCHS = 100\n",
        "# Start training your model\n",
        "elmanR128_history = elmanR128.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback,early_stop_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtZcQl4CaZ-q",
        "outputId": "7de58ea3-edf9-4538-e431-ce68f783ab81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, None, 256)         16896     \n",
            "                                                                 \n",
            " simple_rnn_1 (SimpleRNN)    (None, None, 128)         49280     \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, None, 66)          8514      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 74690 (291.76 KB)\n",
            "Trainable params: 74690 (291.76 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "123/123 [==============================] - 17s 103ms/step - loss: 2.8727\n",
            "Epoch 2/100\n",
            "123/123 [==============================] - 14s 100ms/step - loss: 2.2378\n",
            "Epoch 3/100\n",
            "123/123 [==============================] - 14s 99ms/step - loss: 2.0771\n",
            "Epoch 4/100\n",
            "123/123 [==============================] - 14s 99ms/step - loss: 1.9730\n",
            "Epoch 5/100\n",
            "123/123 [==============================] - 14s 98ms/step - loss: 1.8981\n",
            "Epoch 6/100\n",
            "123/123 [==============================] - 14s 99ms/step - loss: 1.8433\n",
            "Epoch 7/100\n",
            "123/123 [==============================] - 17s 106ms/step - loss: 1.8005\n",
            "Epoch 8/100\n",
            "123/123 [==============================] - 14s 99ms/step - loss: 1.7670\n",
            "Epoch 9/100\n",
            "123/123 [==============================] - 14s 98ms/step - loss: 1.7394\n",
            "Epoch 10/100\n",
            "123/123 [==============================] - 14s 100ms/step - loss: 1.7165\n",
            "Epoch 11/100\n",
            "123/123 [==============================] - 15s 98ms/step - loss: 1.6970\n",
            "Epoch 12/100\n",
            "123/123 [==============================] - 14s 97ms/step - loss: 1.6806\n",
            "Epoch 13/100\n",
            "123/123 [==============================] - 14s 99ms/step - loss: 1.6659\n",
            "Epoch 14/100\n",
            "123/123 [==============================] - 14s 100ms/step - loss: 1.6533\n",
            "Epoch 15/100\n",
            "123/123 [==============================] - 14s 100ms/step - loss: 1.6418\n",
            "Epoch 16/100\n",
            "123/123 [==============================] - 14s 99ms/step - loss: 1.6317\n",
            "Epoch 17/100\n",
            "123/123 [==============================] - 14s 99ms/step - loss: 1.6223\n",
            "Epoch 18/100\n",
            "123/123 [==============================] - 14s 99ms/step - loss: 1.6143\n",
            "Epoch 19/100\n",
            "123/123 [==============================] - 14s 101ms/step - loss: 1.6061\n",
            "Epoch 20/100\n",
            "123/123 [==============================] - 14s 100ms/step - loss: 1.5992\n",
            "Epoch 21/100\n",
            "123/123 [==============================] - 14s 100ms/step - loss: 1.5929\n",
            "Epoch 22/100\n",
            "123/123 [==============================] - 14s 98ms/step - loss: 1.5870\n",
            "Epoch 23/100\n",
            "123/123 [==============================] - 14s 100ms/step - loss: 1.5811\n",
            "Epoch 24/100\n",
            " 92/123 [=====================>........] - ETA: 3s - loss: 1.5779"
          ]
        }
      ]
    }
  ]
}