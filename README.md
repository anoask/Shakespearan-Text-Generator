# Shakespearan-Text-Generator
RNN Creation and Optimization for Text Generation
Assignment Objective:
The objective of this assignment was to create a Shakespearan Text Generator using a GRU RNN, Custom LSTM RNN, and an Elman RNN and compare their results. To optimize the output, create a Beam Search that will be used to find the best output generated.
Implementation Steps:
The GRU implementation was provided, so the first thing we did was create Beam Search. We did this by looking at the provided One_Step class and extracting the and modifying it to be more of a function. BeamSearch(RNN, start_sequence:str, beam_width:int, temperature = 1.0, gen_length=1000): returned a sorted list of words generated and their probabilities in most probable to least probable order. Within it contains a child method one_step(RNN,inputs) with the @tf.function declaration preceding it, and is what retrieves the predicted_logits from the provided RNN model for text generation. We then apply tf.nn.softmax to the predicted_logits to normalize their probability, and use tf.nn.top_k on the softmax to find the top N likely classes and their probabilities where N is our beam_width. This is done under a while condition which when complete returns the most probable text generated by the provided RNN found by BeamSearch(which doesn’t mean it’s the most probable overall).
The second step was implementing the custom LSTM Cell. In retrospect not a difficult task but took us hours of troubleshooting because we were missing “return_sequences=True” in our lstm_layer declaration. Other than that, it was overall very easy to do.
The third step was to implement the Elman RNN model. This ended up being very easy to do, as SimpleRNN in Keras is an Elman RNN, and the activation is defaulted to tanh, which is what the Elman RNN uses. So I basically copied and pasted the custom LSTM implementation, but replaced the LSTM with SimpleRNN.
The fourth step was optimization. Early on while testing BeamSearch on the GRU, everything returned was a bunch of repeating text(Seen Below). Because we were originally unsure what was causing this, we increased EPOCH size from 20 to 100 and enabled an early stopping callback to protect from overtraining. This helped significantly and is the epoch size we used for LSTM and Elman as well. To optimize further, we tried increasing the embedding dimension from 256 to 512. This had a relatively small effect on decreasing loss, for example for our LSTM model it decreased loss from 1.18 to 1.14 with all other parameters the same. Increasing the number of RNN units from 256 to 512 had a very significant effect on decreasing loss in all of our models, for example with our GRU model it decreased loss from 1.1 to 0.69. Decreasing batch_size from 64 to 32 also had an effect on minimizing loss, however, it increased training time too much for our final models to utilize.
